# Advanced Time Series Forecasting with Deep Learning and Attention

This project implements a complete real-world workflow for **multivariate time series forecasting** using a **PyTorch LSTM with a Self-Attention mechanism**, compared against a **baseline LSTM**.

## ğŸš€ Features
- Programmatically generated multivariate dataset (5 features Ã— 5000 steps)
- Data preprocessing and scaling
- Sequence generation for supervised learning
- Two forecasting models:
  - **LSTM + Self-Attention**
  - **Vanilla LSTM**
- Hyperparameter configuration
- Evaluation using RMSE, MAE, MAPE
- Attention weight extraction & visualization
- Clear, modular Python codebase

---

## ğŸ“‚ Project Structure
```
advanced_time_series_project/
â”‚â”€â”€ generate_data.py
â”‚â”€â”€ model.py
â”‚â”€â”€ utils.py
â”‚â”€â”€ train.py
â”‚â”€â”€ evaluate.py
â”‚â”€â”€ report.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ data.csv (generated)
â”‚â”€â”€ attention_weights.npy (after training)
```

---

## ğŸ›  Installation
```bash
pip install -r requirements.txt
```

---

## ğŸ“Š Generate Dataset
```bash
python generate_data.py
```

---

## ğŸ§  Train Models
```bash
python train.py
```

This trains:
- âœ” LSTM with Attention  
- âœ” Vanilla LSTM  
and saves results as:

- `results.json`
- `best_model.pth`
- `attention_weights.npy`

---

## ğŸ“ˆ Evaluate & Visualize
```bash
python evaluate.py
```

If attention weights are present, this generates:
- `attention_weights.png`

---

## ğŸ“Œ Notes
- Increase epochs (50â€“200) for real accuracy.
- GPU recommended for faster training.
- You can extend this to multi-step forecasting or add other baselines (ARIMA, Prophet, VAR).

---

## ğŸ“ License
MIT License.
